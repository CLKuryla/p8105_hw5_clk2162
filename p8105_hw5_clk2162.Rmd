---
title: "P8105 Homework 5"
author: "Christine Lucille Kuryla (clk2162)"
date: "2023-11-15"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1

## Import and clean data

This problem explores the proportion of homicides in several cities that are solved/result in arrest. Let's start by importing, cleaning, and manipulating the data to serve our purposes.

```{r import_clean_data}
homicide_data <- 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe `homicide_data` has information on `r colnames(homicide_data)` and has been cleaned. 

## Proportion of unsolved homicides in Baltimore

```{r prop_baltimore}

# Proportion test to estimite the proportion of unsolved homicides in Baltimore

# filter and select relevant data

baltimore <- homicide_data %>%
  filter(city_state == "Baltimore, MD") %>%
  select(resolution) %>%
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved")
  )

# perform proportion test

baltimore_prop <- 
    prop.test(
    x = baltimore %>% pull(hom_unsolved),
    n = baltimore %>% pull(hom_total)) 

broom::tidy(baltimore_prop) %>% 
  knitr::kable(digits = 3)

```

The proportion of unsolved homicides in Baltimore is  `r baltimore_prop$estimate`. See above table for details. 

## Proportion of unsolved homicides for all cities in dataset

Note, this code is adapted from the solutions. 

```{r}

# Create a df summarizing the total and unsolved number of homicides in all the cities in the dataset

city_homicide_df <- 
  homicide_data %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved")) 

# Run proportion test on all cities and put in a new df

prop_all_cities_results <- 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) 

# plot results and CIs

prop_all_cities_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

# Problem 2

This problem analyzes a control and experimental arm over time, where the subjects' data is in separate files. 

```{r data_import_wrangle, message=FALSE}
# import and combine data

files <- list.files("./data/data-3/")

# create a table in a for loop

output <- vector("list", length = length(files))
for (i in 1:length(files)) {
  output[[i]] =  read_csv(str_c("./data/data-3/", files[i]))
}

# let's do this with purr:map as well as including the name as a corresponding list

experiment_df <- tibble(
  file_name = files,
  subj_data = map_df(file_name, \(x) read_csv(str_c("./data/data-3/", x)))  
)

experiment_df <- experiment_df %>% 
  unnest(subj_data) %>% 
  mutate(group = str_sub(file_name, 1, 3)) %>% 
  mutate(id = str_sub(file_name, 1, 6)) %>% 
  mutate(group = case_when(
      group == "con" ~ "control",
      group == "exp" ~ "experimental"
      )) %>% 
  pivot_longer(cols = week_1:week_8,
               names_to = "week",
               values_to = "value") %>% 
  mutate(week = str_sub(week, 6,6)) %>% 
  mutate(week = as.numeric(week)) %>% 
  select(-file_name) %>% 
  relocate(id)

```

The data has been imported by itering over file name using a function, then extracting the values for each week from the imported df, then extracting the group from the name of the file, and finally cleaned and tidied. The final dataframe has the columns `r colnames(experiment_df)`.

Now we will visualize the data.

```{r plot, message=FALSE}

# spaghetti plot of observations on each subject over time

experiment_df %>% 
  group_by(id) %>% 
  ggplot(aes(x = week, y = value,  color = group, group = id)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Spaghetti plot of value over time",
    x = "Week",
    y = "Observed Value"
  )

```

The experimental group's observed value seems to increase over time and the control group's value seems relatively constant. The intervention probably increases whatever the outcome variable is.

# Problem 3

Prompt: When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a one-sample t-test. 
First set the following design elements:

Fix n=30
Fix σ=5
Set μ=0
. Generate 5000 datasets from the model

x∼Normal[μ,σ]

For each dataset, save μ̂
 and the p-value arising from a test of H:μ=0
 using α=0.05
. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.

Repeat the above for μ={1,2,3,4,5,6}
, and complete the following:

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ
on the x axis. Describe the association between effect size and power.
Make a plot showing the average estimate of μ̂
 on the y axis and the true value of μ
 on the x axis. Make a second plot (or overlay on the first) the average estimate of μ̂
 only in samples for which the null was rejected on the y axis and the true value of μ
 on the x axis. Is the sample average of μ̂
 across tests for which the null is rejected approximately equal to the true value of μ
? Why or why not?

```{r q3, eval = FALSE}

# Simulate model repeatedly
# The model is a normal distribution with mean 0 and sd 5
# We will sample 30 values each sampling

normal_sample <- function() {
  
  # simulate data
  sample <- rnorm(n = 30, mean = 0, sd = 5)
  
  result <- 
    
  
  mean_hat <- mean(sample)
  p_value <- broom::tidy(t.test(sample))
}

list(
  sample = rnorm(n = 30, mean = 0, sd = 5),
  mean_hat = summarize(mean(sample))
)




```

